## 6.2 Stochastic Gradient Descent
### SGD: Motivation
In machine learning, often the objective function is of the type $$f(x) = E f(x, \cdot) = \int_\Omega f(x, \omega)\,d P(\omega)$$
where $f(x): \Omega \to \mathbb R$ are random variables. Therefore the gradient is
$$\nabla f(x) = \int_\Omega \nabla f(x, \omega)\,dP(\omega).$$
(Concretely: Often $\Omega$ represents the training data).

To reduce the complexity of computing this expected value, one estimates it to reduce the complexity.

### Stochastic Gradient Descent
In each iteration, pick $\omega^{(n)}$ at random and use the update rule
$$x^{(n+1)} = x^{(n)} - \alpha \nabla_x f(x^{(n)}, 
\, \omega{(n)})$$

> **Theorem 19.1**
> Assume that
> - $f(\cdot, \omega)$ is convex almost surely,
> - $\nabla_x f(x, \omega)$ is $L$-Lipschitz with $L < L_{max}$ almost surely, and
> - $f(x) = Ef(x, \cdot)$ is $\gamma$-strongly convex.
> 
> Let $\sigma^2 = E||\nabla_x f(x^\ast, \cdot)||_2^2$, where $x^\ast$ minimizes $f$. Suppose $\alpha < L_{max}$. Then the stochastic gradient descent iteration satisfies:
> $$E ||x^{(n)} - x^\ast||_2^2 \leq (1-2\alpha\gamma(1-\alpha L_{max}))^n ||x^{(0)} - x^\ast||_2^2 + \frac{\alpha \sigma^2}{\gamma (1-\alpha L_{max})}$$
> The expectation is taken over the sequence of sampling $(\omega{(n)})_n$.

^e45a97

Intuitively, the n-th estimation is at most a constant distance plus an exponentially decaying term away from the optimum if we choose a suitable $\alpha$ depending on $\gamma$ and $L_{max}$ (and $\sigma^2$ as well regarding the constant distance).

%% #Lecture 22, 26.07. 
[[6.2 Numerical Optimization - Stochastic Gradient Descent]] %%

###### #Proof of [[#^e45a97|Theorem 19.1]]
Denote $f_1(x) := f(x, \omega^{(n)})$. Then $$||x^{n+1} - x^\ast||_2^2 = ||x^{(n)} - x^\ast - \alpha \nabla_x f_1(x^{(n)})||_2^2$$ $$= ||(x^{(n)} - x^\ast) - \alpha (\nabla_x f_n(x^{(n)}) - \nabla_x f_n(x^\ast) + \nabla_x f_1(x^\ast)) ||_2^2$$ $$=||x^{(1)} - x^\ast||_2^2 - 2^\alpha \langle x^{(n)} - x^\ast, \nabla_x f_n(x^{(n)}) \rangle + \alpha^2 ||\nabla_x f_n(x^{(n)}) - \nabla_x f_n(x^\ast) + \nabla_x f_n(x^\ast)||_2^2$$ $$\leq ||x^{(n)} - x^\ast||_2^2 - 2\alpha \langle x^{(n)} - x^\ast, \nabla_x f_n(x^{(n)}) \rangle + 2\alpha^2 ||\nabla_x f_n(x^{(n)} - \nabla_x f_n(x^\ast)||_2^2 + 2\alpha ||\nabla_x f_n(x^\ast)||_2^2$$
(where we used that $(a+b)^2 = 2a^2+2b^2 - (a-b)^2 \leq 2a^2 + 2b^2$ - "up to factor 2, for n=2 the freshman's dream is true"). Now use [[6.1 TODO - Numerical Optimization - Gradient Descent#^505679|Co-Coercivity Lemma]]:

$$\leq ||x^{(n)} - x^\ast||_2^2 - 2\alpha \langle x^{(n)} - x^\ast, \nabla_x f_n(x^{(n)})\rangle + 2\alpha^2 L_{\max} \langle x-x^\ast, \nabla_x f_n(x^{(n)}) - \nabla_x f_n(x^\ast)\rangle + 2\alpha^2 ||\nabla f_1(x^\ast)||_2^2$$
Take the expectation conditioned on $x^{(n)}$ on both sides to get:

$$E (||x^{(n+1)} - x^\ast||_2^2 | x^{(n)}
\leq ||x^{(n)} - x^\ast||_2^2 - 2\alpha \langle x^{(n)} - x^\ast, \nabla_x f_n(x^{(n)})\rangle + 2\alpha^2 L_{\max} E(\langle x-x^\ast, \nabla_x f_n(x^{(n)}) - \nabla_x f_n(x^\ast)\rangle | x^{(n)}) + 2\alpha^2 E(||\nabla f_1(x^\ast)||_2^2)$$

$$\leq ||x^{(n)} - x^\ast||_2^2 - 2\alpha \langle x^{(n)} - x^\ast, \nabla_x f_n(x^{(n)})\rangle + 2\alpha^2 L_{\max} E(\langle x-x^\ast, \nabla_x f_n(x^{(n)}) - \nabla_x f_n(x^\ast)\rangle | x^{(n)}) + 2\alpha^2 \sigma^2$$

Now, calculating the expected value by taking into account the "hidden" randomness in $f_n$ and exploit strong convexity (details omitted):

$$\leq ||x^{(n)} - x^\ast||-2^2 - 2\alpha \gamma (1-\alpha L_{\max})||x^{(n)} - x^\ast||_2^2 + 2\alpha^2 \sigma^2$$
$$= (1-2\alpha \gamma (1-\alpha L_{\max}) ||x^{(n)} - x^\ast|| + 2\alpha \sigma^2)$$
whenever $\alpha < 1/L_{\max}$.

This almost completes the proof: all that's left is to recursively apply the new bound over the first $n$ iterations to yield the desired result. Namely,
$$E(||x^{(n+1)} - x^\ast||_2^2) \leq (1-2\alpha \gamma (1-\alpha L_{\max}))^{\dots} \cdot (\dots) + \dots$$